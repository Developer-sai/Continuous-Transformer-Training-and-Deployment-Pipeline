{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toJorDT-x4aB",
    "outputId": "03dad8af-d0fd-44ec-c57b-465d4d8c230c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: mlflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mlflow-skinny==2.17.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.17.1)\n",
      "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.3)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.4.1)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
      "Requirement already satisfied: pyarrow<18,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (16.1.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.36)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n",
      "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.10/dist-packages (from mlflow) (23.0.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (3.1.0)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (0.36.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (3.1.43)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (1.16.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (3.20.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.6)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.2.3)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.0.6)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.5)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
      "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow) (2.27.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.17.1->mlflow) (4.0.11)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.17.1->mlflow) (3.20.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow) (1.2.14)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow) (75.1.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow) (0.37b0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.17.1->mlflow) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers pandas scikit-learn mlflow torch datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S63C5m_4x4D9",
    "outputId": "1ac3f6fa-05f1-45be-fc08-ad41cdd98af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///content/mlruns/544208329798821324', creation_time=1730313347843, experiment_id='544208329798821324', last_update_time=1730313347843, lifecycle_stage='active', name='Text_Classification_Retraining', tags={}>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Disable W&B tracking to avoid unnecessary warnings\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Set up MLflow for local tracking\n",
    "mlflow.set_tracking_uri(\"file:///content/mlruns\")  # Local MLflow tracking\n",
    "mlflow.set_experiment(\"Text_Classification_Retraining\")  # Experiment name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "2SH3dudbx8xl"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "-LYYEcMayAKE",
    "outputId": "a077f73b-13ba-4992-c555-65694ada08c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-35b115b9-34c7-443a-b096-308c94cad28a\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-35b115b9-34c7-443a-b096-308c94cad28a\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sentimentdataset.csv to sentimentdataset.csv\n",
      "sentimentdataset.csv uploaded successfully!\n",
      "   Unnamed: 0.1  Unnamed: 0  \\\n",
      "0             0           0   \n",
      "1             1           1   \n",
      "2             2           2   \n",
      "3             3           3   \n",
      "4             4           4   \n",
      "\n",
      "                                                Text    Sentiment  \\\n",
      "0   Enjoying a beautiful day at the park!        ...   Positive     \n",
      "1   Traffic was terrible this morning.           ...   Negative     \n",
      "2   Just finished an amazing workout! 💪          ...   Positive     \n",
      "3   Excited about the upcoming weekend getaway!  ...   Positive     \n",
      "4   Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
      "\n",
      "             Timestamp            User     Platform  \\\n",
      "0  2023-01-15 12:30:00   User123          Twitter     \n",
      "1  2023-01-15 08:45:00   CommuterX        Twitter     \n",
      "2  2023-01-15 15:45:00   FitnessFan      Instagram    \n",
      "3  2023-01-15 18:20:00   AdventureX       Facebook    \n",
      "4  2023-01-15 19:55:00   ChefCook        Instagram    \n",
      "\n",
      "                                     Hashtags  Retweets  Likes       Country  \\\n",
      "0   #Nature #Park                                  15.0   30.0     USA         \n",
      "1   #Traffic #Morning                               5.0   10.0     Canada      \n",
      "2   #Fitness #Workout                              20.0   40.0   USA           \n",
      "3   #Travel #Adventure                              8.0   15.0     UK          \n",
      "4   #Cooking #Food                                 12.0   25.0    Australia    \n",
      "\n",
      "   Year  Month  Day  Hour  \n",
      "0  2023      1   15    12  \n",
      "1  2023      1   15     8  \n",
      "2  2023      1   15    15  \n",
      "3  2023      1   15    18  \n",
      "4  2023      1   15    19  \n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload the dataset\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load dataset\n",
    "for file_name in uploaded.keys():\n",
    "    data = pd.read_csv(file_name)\n",
    "    print(f\"{file_name} uploaded successfully!\")\n",
    "    print(data.head())  # Display a preview of the dataset\n",
    "\n",
    "# Define text and label columns based on your dataset structure\n",
    "X = data['Text']  # Replace with your actual text column name if different\n",
    "y = data['Sentiment']  # Replace with your actual label column name if different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7C0Cff4yDtL",
    "outputId": "7715a2c7-875c-40fa-c9e2-69dc10ffb271"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to numeric format if they are not already\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_data(texts, labels, tokenizer_name='distilbert-base-uncased', max_length=128):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "    return TextClassificationDataset(encodings, labels)  # Use custom dataset class\n",
    "\n",
    "# Preprocess train and test data\n",
    "train_dataset = preprocess_data(X_train, y_train)\n",
    "test_dataset = preprocess_data(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "S9uBRICNyFia"
   },
   "outputs": [],
   "source": [
    "def train_model_with_mlflow(train_dataset, test_dataset, model_name='distilbert-base-uncased', epochs=3):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "    # Log parameters with MLflow\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            evaluation_strategy=\"epoch\",  # Use \"epoch\" to evaluate at each epoch\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            report_to=\"none\"  # Disable W&B logging\n",
    "        )\n",
    "\n",
    "        # Trainer setup with custom dataset\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset\n",
    "        )\n",
    "\n",
    "        # Train and evaluate\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        accuracy = eval_results.get('eval_accuracy', 0.0)  # Default to 0.0 if accuracy is missing\n",
    "\n",
    "        # Log metrics and model artifacts to MLflow\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.pytorch.log_model(model, \"text_classification_model\")\n",
    "\n",
    "    return model, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8BORAVuqyH4e"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Set up the classifier pipeline with `top_k=None` instead of `return_all_scores=True`\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=\"distilbert-base-uncased\", top_k=None)\n",
    "    predictions = classifier(X_test.tolist(), batch_size=16)\n",
    "\n",
    "    # Extract numeric label ID from 'LABEL_x' format and convert to int\n",
    "    pred_labels = [int(pred[0]['label'].split('_')[-1]) for pred in predictions]\n",
    "    accuracy = accuracy_score(y_test, pred_labels)\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, pred_labels))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "wvgbFQFUyKHs"
   },
   "outputs": [],
   "source": [
    "def monitor_and_retrain_with_mlflow(train_dataset, y_train, model, current_accuracy, threshold=0.80):\n",
    "    try:\n",
    "        last_run = mlflow.search_runs(order_by=[\"start_time desc\"], max_results=1)\n",
    "        last_accuracy = last_run['metrics.accuracy'].values[0] if 'metrics.accuracy' in last_run.columns else 1.0\n",
    "\n",
    "        # Ensure current_accuracy and last_accuracy are valid numbers\n",
    "        current_accuracy = current_accuracy if current_accuracy is not None else 0.0\n",
    "        last_accuracy = last_accuracy if last_accuracy is not None else 1.0\n",
    "\n",
    "        if current_accuracy < threshold * last_accuracy:\n",
    "            print(\"Retraining triggered due to accuracy drop.\")\n",
    "            model, new_accuracy = train_model_with_mlflow(train_dataset, test_dataset)\n",
    "            return model, new_accuracy\n",
    "    except KeyError:\n",
    "        print(\"Accuracy metric not found in MLflow run logs. Skipping retraining check.\")\n",
    "\n",
    "    return model, current_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "60Wn-AoTyMU1"
   },
   "outputs": [],
   "source": [
    "def live_inference(model, tokenizer, text):\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    result = classifier(text)\n",
    "    predicted_label = result[0]['label']\n",
    "    label_id = int(predicted_label.split(\"_\")[-1])  # Get label ID from prediction format\n",
    "    actual_label = label_encoder.inverse_transform([label_id])[0]  # Map ID to original label\n",
    "    print(f\"Predicted label: {actual_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "hBleKZmYyPtS",
    "outputId": "07624413-17dd-4970-d0bb-de922f364ea1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [111/111 00:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.421296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.332127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.289758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/30 18:59:21 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.0+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/30 18:59:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy: 0.0\n",
      "Enter text for live inference: i went to hospital for treatment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label:  Positive  \n"
     ]
    }
   ],
   "source": [
    "# Initial training\n",
    "model, initial_accuracy = train_model_with_mlflow(train_dataset, test_dataset)\n",
    "\n",
    "# Evaluate and monitor with error handling\n",
    "model, final_accuracy = monitor_and_retrain_with_mlflow(train_dataset, y_train, model, initial_accuracy)\n",
    "print(\"Final Model Accuracy:\", final_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P64IemWky9i2",
    "outputId": "806f5dd5-60b1-4e72-cb49-01928bb3e6fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text for live inference: i went to hospital for tretment i'm so sick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label:  Positive  \n"
     ]
    }
   ],
   "source": [
    "# Live inference usage with input()\n",
    "test_text = input(\"Enter text for live inference: \")  # Prompt for user input\n",
    "live_inference(model, AutoTokenizer.from_pretrained('distilbert-base-uncased'), test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import joblib\n",
    "import smtplib\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Configurations\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable W&B tracking\n",
    "DATA_PATH = '/content/sentimentdataset.csv'\n",
    "MLFLOW_URI = \"file:///content/mlruns\"  # MLflow local tracking URI\n",
    "MLFLOW_EXPERIMENT = \"Text_Classification_Retraining\"\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LOGGING_STEPS = 10\n",
    "OUTPUT_DIR = './results'\n",
    "LABEL_ENCODER_PATH = 'label_encoder.pkl'\n",
    "STALENESS_THRESHOLD = 0.75  # Accuracy threshold for staleness\n",
    "EMAIL_ALERT = \"admin@example.com\"  # Email for alerts\n",
    "MONITORING_INTERVAL = 3600  # Check every hour (3600 seconds)\n",
    "\n",
    "# MLflow Setup\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "app = Flask(__name__)  # Flask app for live inference\n",
    "\n",
    "# Email Alert Setup\n",
    "def send_alert_email(metric, current_value):\n",
    "    with smtplib.SMTP(\"smtp.gmail.com\", 587) as smtp:\n",
    "        smtp.starttls()\n",
    "        smtp.login(\"your_email@gmail.com\", \"your_password\")\n",
    "        subject = \"Model Retraining Alert\"\n",
    "        body = f\"The model's {metric} has fallen below the threshold: {current_value}. Retraining is triggered.\"\n",
    "        msg = f'Subject: {subject}\\n\\n{body}'\n",
    "        smtp.sendmail(\"your_email@gmail.com\", EMAIL_ALERT, msg)\n",
    "\n",
    "# Dataset Class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {'accuracy': accuracy, 'f1': f1}\n",
    "\n",
    "# Training and Retraining Function\n",
    "def train_and_log_model(train_texts, val_texts, train_labels, val_labels):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
    "    \n",
    "    train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
    "    \n",
    "    train_dataset = TextClassificationDataset(train_encodings, train_labels.tolist())\n",
    "    val_dataset = TextClassificationDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Log metrics with MLflow\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params({\"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE, \"model_name\": MODEL_NAME})\n",
    "        mlflow.log_metrics(eval_results)\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    # Save the model and check for staleness\n",
    "    joblib.dump(label_encoder, LABEL_ENCODER_PATH)\n",
    "    if eval_results['eval_accuracy'] < STALENESS_THRESHOLD:\n",
    "        send_alert_email(\"accuracy\", eval_results['eval_accuracy'])\n",
    "    return model, eval_results\n",
    "\n",
    "# Monitoring and Retraining Loop\n",
    "def monitoring_and_retrain():\n",
    "    global model, val_texts, val_labels\n",
    "    while True:\n",
    "        print(\"Monitoring model performance...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
    "        val_dataset = TextClassificationDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "        # Perform evaluation\n",
    "        trainer = Trainer(model=model, eval_dataset=val_dataset, compute_metrics=compute_metrics)\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        # Check staleness and retrain if necessary\n",
    "        if eval_results['eval_accuracy'] < STALENESS_THRESHOLD:\n",
    "            print(\"Performance degraded. Retraining the model...\")\n",
    "            model, eval_results = train_and_log_model(train_texts, val_texts, train_labels, val_labels)\n",
    "\n",
    "        print(\"Monitoring completed. Waiting for the next interval...\")\n",
    "        time.sleep(MONITORING_INTERVAL)\n",
    "\n",
    "# Initial Training\n",
    "data = pd.read_csv(DATA_PATH)[['text', 'label']]\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['label'])\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['text'], data['label'], test_size=0.2)\n",
    "model, _ = train_and_log_model(train_texts, val_texts, train_labels, val_labels)\n",
    "\n",
    "# Batch Inference\n",
    "def batch_inference(texts):\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model(**encodings)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "    return label_encoder.inverse_transform(predictions.cpu().numpy())\n",
    "\n",
    "# Live Inference\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def live_inference():\n",
    "    data = request.json\n",
    "    text = data['text']\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    output = model(**encoding)\n",
    "    prediction = torch.argmax(output.logits, dim=1)\n",
    "    predicted_label = label_encoder.inverse_transform([prediction.item()])[0]\n",
    "    return jsonify({'prediction': predicted_label})\n",
    "\n",
    "# Start monitoring in a background thread\n",
    "import threading\n",
    "monitoring_thread = threading.Thread(target=monitoring_and_retrain)\n",
    "monitoring_thread.daemon = True\n",
    "monitoring_thread.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
