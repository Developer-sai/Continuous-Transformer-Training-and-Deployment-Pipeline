{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toJorDT-x4aB",
    "outputId": "03dad8af-d0fd-44ec-c57b-465d4d8c230c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: mlflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mlflow-skinny==2.17.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.17.1)\n",
      "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.3)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.4.1)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
      "Requirement already satisfied: pyarrow<18,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (16.1.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.36)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n",
      "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.10/dist-packages (from mlflow) (23.0.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (3.1.0)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (0.36.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (3.1.43)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (1.16.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (3.20.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.1->mlflow) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.6)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.2.3)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.0.6)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.5)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
      "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow) (2.27.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.17.1->mlflow) (4.0.11)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.17.1->mlflow) (3.20.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow) (1.2.14)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow) (75.1.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow) (0.37b0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.17.1->mlflow) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers pandas scikit-learn mlflow torch datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S63C5m_4x4D9",
    "outputId": "1ac3f6fa-05f1-45be-fc08-ad41cdd98af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///content/mlruns/544208329798821324', creation_time=1730313347843, experiment_id='544208329798821324', last_update_time=1730313347843, lifecycle_stage='active', name='Text_Classification_Retraining', tags={}>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Disable W&B tracking to avoid unnecessary warnings\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Set up MLflow for local tracking\n",
    "mlflow.set_tracking_uri(\"file:///content/mlruns\")  # Local MLflow tracking\n",
    "mlflow.set_experiment(\"Text_Classification_Retraining\")  # Experiment name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "2SH3dudbx8xl"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "-LYYEcMayAKE",
    "outputId": "a077f73b-13ba-4992-c555-65694ada08c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-35b115b9-34c7-443a-b096-308c94cad28a\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-35b115b9-34c7-443a-b096-308c94cad28a\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sentimentdataset.csv to sentimentdataset.csv\n",
      "sentimentdataset.csv uploaded successfully!\n",
      "   Unnamed: 0.1  Unnamed: 0  \\\n",
      "0             0           0   \n",
      "1             1           1   \n",
      "2             2           2   \n",
      "3             3           3   \n",
      "4             4           4   \n",
      "\n",
      "                                                Text    Sentiment  \\\n",
      "0   Enjoying a beautiful day at the park!        ...   Positive     \n",
      "1   Traffic was terrible this morning.           ...   Negative     \n",
      "2   Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
      "3   Excited about the upcoming weekend getaway!  ...   Positive     \n",
      "4   Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
      "\n",
      "             Timestamp            User     Platform  \\\n",
      "0  2023-01-15 12:30:00   User123          Twitter     \n",
      "1  2023-01-15 08:45:00   CommuterX        Twitter     \n",
      "2  2023-01-15 15:45:00   FitnessFan      Instagram    \n",
      "3  2023-01-15 18:20:00   AdventureX       Facebook    \n",
      "4  2023-01-15 19:55:00   ChefCook        Instagram    \n",
      "\n",
      "                                     Hashtags  Retweets  Likes       Country  \\\n",
      "0   #Nature #Park                                  15.0   30.0     USA         \n",
      "1   #Traffic #Morning                               5.0   10.0     Canada      \n",
      "2   #Fitness #Workout                              20.0   40.0   USA           \n",
      "3   #Travel #Adventure                              8.0   15.0     UK          \n",
      "4   #Cooking #Food                                 12.0   25.0    Australia    \n",
      "\n",
      "   Year  Month  Day  Hour  \n",
      "0  2023      1   15    12  \n",
      "1  2023      1   15     8  \n",
      "2  2023      1   15    15  \n",
      "3  2023      1   15    18  \n",
      "4  2023      1   15    19  \n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload the dataset\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load dataset\n",
    "for file_name in uploaded.keys():\n",
    "    data = pd.read_csv(file_name)\n",
    "    print(f\"{file_name} uploaded successfully!\")\n",
    "    print(data.head())  # Display a preview of the dataset\n",
    "\n",
    "# Define text and label columns based on your dataset structure\n",
    "X = data['Text']  # Replace with your actual text column name if different\n",
    "y = data['Sentiment']  # Replace with your actual label column name if different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7C0Cff4yDtL",
    "outputId": "7715a2c7-875c-40fa-c9e2-69dc10ffb271"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to numeric format if they are not already\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_data(texts, labels, tokenizer_name='distilbert-base-uncased', max_length=128):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "    return TextClassificationDataset(encodings, labels)  # Use custom dataset class\n",
    "\n",
    "# Preprocess train and test data\n",
    "train_dataset = preprocess_data(X_train, y_train)\n",
    "test_dataset = preprocess_data(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "S9uBRICNyFia"
   },
   "outputs": [],
   "source": [
    "def train_model_with_mlflow(train_dataset, test_dataset, model_name='distilbert-base-uncased', epochs=3):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "    # Log parameters with MLflow\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            evaluation_strategy=\"epoch\",  # Use \"epoch\" to evaluate at each epoch\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            report_to=\"none\"  # Disable W&B logging\n",
    "        )\n",
    "\n",
    "        # Trainer setup with custom dataset\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset\n",
    "        )\n",
    "\n",
    "        # Train and evaluate\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        accuracy = eval_results.get('eval_accuracy', 0.0)  # Default to 0.0 if accuracy is missing\n",
    "\n",
    "        # Log metrics and model artifacts to MLflow\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.pytorch.log_model(model, \"text_classification_model\")\n",
    "\n",
    "    return model, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8BORAVuqyH4e"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Set up the classifier pipeline with `top_k=None` instead of `return_all_scores=True`\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=\"distilbert-base-uncased\", top_k=None)\n",
    "    predictions = classifier(X_test.tolist(), batch_size=16)\n",
    "\n",
    "    # Extract numeric label ID from 'LABEL_x' format and convert to int\n",
    "    pred_labels = [int(pred[0]['label'].split('_')[-1]) for pred in predictions]\n",
    "    accuracy = accuracy_score(y_test, pred_labels)\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, pred_labels))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "wvgbFQFUyKHs"
   },
   "outputs": [],
   "source": [
    "def monitor_and_retrain_with_mlflow(train_dataset, y_train, model, current_accuracy, threshold=0.80):\n",
    "    try:\n",
    "        last_run = mlflow.search_runs(order_by=[\"start_time desc\"], max_results=1)\n",
    "        last_accuracy = last_run['metrics.accuracy'].values[0] if 'metrics.accuracy' in last_run.columns else 1.0\n",
    "\n",
    "        # Ensure current_accuracy and last_accuracy are valid numbers\n",
    "        current_accuracy = current_accuracy if current_accuracy is not None else 0.0\n",
    "        last_accuracy = last_accuracy if last_accuracy is not None else 1.0\n",
    "\n",
    "        if current_accuracy < threshold * last_accuracy:\n",
    "            print(\"Retraining triggered due to accuracy drop.\")\n",
    "            model, new_accuracy = train_model_with_mlflow(train_dataset, test_dataset)\n",
    "            return model, new_accuracy\n",
    "    except KeyError:\n",
    "        print(\"Accuracy metric not found in MLflow run logs. Skipping retraining check.\")\n",
    "\n",
    "    return model, current_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "60Wn-AoTyMU1"
   },
   "outputs": [],
   "source": [
    "def live_inference(model, tokenizer, text):\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    result = classifier(text)\n",
    "    predicted_label = result[0]['label']\n",
    "    label_id = int(predicted_label.split(\"_\")[-1])  # Get label ID from prediction format\n",
    "    actual_label = label_encoder.inverse_transform([label_id])[0]  # Map ID to original label\n",
    "    print(f\"Predicted label: {actual_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "hBleKZmYyPtS",
    "outputId": "07624413-17dd-4970-d0bb-de922f364ea1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [111/111 00:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.421296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.332127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.289758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/30 18:59:21 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.0+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/30 18:59:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy: 0.0\n",
      "Enter text for live inference: i went to hospital for treatment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label:  Positive  \n"
     ]
    }
   ],
   "source": [
    "# Initial training\n",
    "model, initial_accuracy = train_model_with_mlflow(train_dataset, test_dataset)\n",
    "\n",
    "# Evaluate and monitor with error handling\n",
    "model, final_accuracy = monitor_and_retrain_with_mlflow(train_dataset, y_train, model, initial_accuracy)\n",
    "print(\"Final Model Accuracy:\", final_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P64IemWky9i2",
    "outputId": "806f5dd5-60b1-4e72-cb49-01928bb3e6fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text for live inference: i went to hospital for tretment i'm so sick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label:  Positive  \n"
     ]
    }
   ],
   "source": [
    "# Live inference usage with input()\n",
    "test_text = input(\"Enter text for live inference: \")  # Prompt for user input\n",
    "live_inference(model, AutoTokenizer.from_pretrained('distilbert-base-uncased'), test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import joblib\n",
    "import smtplib\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Configurations\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable W&B tracking\n",
    "DATA_PATH = '/content/sentimentdataset.csv'\n",
    "MLFLOW_URI = \"file:///content/mlruns\"  # MLflow local tracking URI\n",
    "MLFLOW_EXPERIMENT = \"Text_Classification_Retraining\"\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LOGGING_STEPS = 10\n",
    "OUTPUT_DIR = './results'\n",
    "LABEL_ENCODER_PATH = 'label_encoder.pkl'\n",
    "STALENESS_THRESHOLD = 0.75  # Accuracy threshold for staleness\n",
    "EMAIL_ALERT = \"admin@example.com\"  # Email for alerts\n",
    "MONITORING_INTERVAL = 3600  # Check every hour (3600 seconds)\n",
    "\n",
    "# MLflow Setup\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "app = Flask(__name__)  # Flask app for live inference\n",
    "\n",
    "# Email Alert Setup\n",
    "def send_alert_email(metric, current_value):\n",
    "    with smtplib.SMTP(\"smtp.gmail.com\", 587) as smtp:\n",
    "        smtp.starttls()\n",
    "        smtp.login(\"your_email@gmail.com\", \"your_password\")\n",
    "        subject = \"Model Retraining Alert\"\n",
    "        body = f\"The model's {metric} has fallen below the threshold: {current_value}. Retraining is triggered.\"\n",
    "        msg = f'Subject: {subject}\\n\\n{body}'\n",
    "        smtp.sendmail(\"your_email@gmail.com\", EMAIL_ALERT, msg)\n",
    "\n",
    "# Dataset Class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {'accuracy': accuracy, 'f1': f1}\n",
    "\n",
    "# Training and Retraining Function\n",
    "def train_and_log_model(train_texts, val_texts, train_labels, val_labels):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
    "    \n",
    "    train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
    "    \n",
    "    train_dataset = TextClassificationDataset(train_encodings, train_labels.tolist())\n",
    "    val_dataset = TextClassificationDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Log metrics with MLflow\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params({\"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE, \"model_name\": MODEL_NAME})\n",
    "        mlflow.log_metrics(eval_results)\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    # Save the model and check for staleness\n",
    "    joblib.dump(label_encoder, LABEL_ENCODER_PATH)\n",
    "    if eval_results['eval_accuracy'] < STALENESS_THRESHOLD:\n",
    "        send_alert_email(\"accuracy\", eval_results['eval_accuracy'])\n",
    "    return model, eval_results\n",
    "\n",
    "# Monitoring and Retraining Loop\n",
    "def monitoring_and_retrain():\n",
    "    global model, val_texts, val_labels\n",
    "    while True:\n",
    "        print(\"Monitoring model performance...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
    "        val_dataset = TextClassificationDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "        # Perform evaluation\n",
    "        trainer = Trainer(model=model, eval_dataset=val_dataset, compute_metrics=compute_metrics)\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        # Check staleness and retrain if necessary\n",
    "        if eval_results['eval_accuracy'] < STALENESS_THRESHOLD:\n",
    "            print(\"Performance degraded. Retraining the model...\")\n",
    "            model, eval_results = train_and_log_model(train_texts, val_texts, train_labels, val_labels)\n",
    "\n",
    "        print(\"Monitoring completed. Waiting for the next interval...\")\n",
    "        time.sleep(MONITORING_INTERVAL)\n",
    "\n",
    "# Initial Training\n",
    "data = pd.read_csv(DATA_PATH)[['text', 'label']]\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['label'])\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['text'], data['label'], test_size=0.2)\n",
    "model, _ = train_and_log_model(train_texts, val_texts, train_labels, val_labels)\n",
    "\n",
    "# Batch Inference\n",
    "def batch_inference(texts):\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model(**encodings)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "    return label_encoder.inverse_transform(predictions.cpu().numpy())\n",
    "\n",
    "# Live Inference\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def live_inference():\n",
    "    data = request.json\n",
    "    text = data['text']\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    output = model(**encoding)\n",
    "    prediction = torch.argmax(output.logits, dim=1)\n",
    "    predicted_label = label_encoder.inverse_transform([prediction.item()])[0]\n",
    "    return jsonify({'prediction': predicted_label})\n",
    "\n",
    "# Start monitoring in a background thread\n",
    "import threading\n",
    "monitoring_thread = threading.Thread(target=monitoring_and_retrain)\n",
    "monitoring_thread.daemon = True\n",
    "monitoring_thread.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
