{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjVx0682nMMD"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'your-bucket-name'\n",
    "file_name = 'your-dataset.csv'\n",
    "\n",
    "# Upload file to S3\n",
    "s3.upload_file(file_name, bucket_name, file_name)\n",
    "print(f\"{file_name} uploaded to S3 bucket {bucket_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwUJCf6unToD"
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "import argparse\n",
    "import os\n",
    "import boto3\n",
    "import mlflow\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Parse input arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--s3-data', type=str, default='')\n",
    "parser.add_argument('--s3-output', type=str, default='')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Load data from S3\n",
    "data = pd.read_csv(args.s3_data)\n",
    "\n",
    "# Define X and y columns (customize as needed)\n",
    "X = data['Text']\n",
    "y = data['Label']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model and trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(set(y)))\n",
    "args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=X_train,\n",
    "    eval_dataset=X_test\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "accuracy = trainer.evaluate()['eval_accuracy']\n",
    "\n",
    "# Log to MLflow\n",
    "mlflow.log_param(\"epochs\", 3)\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "# Save model to S3\n",
    "model.save_pretrained(args.s3_output)\n",
    "print(\"Model training completed and saved to S3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYwucMPYnVLY"
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = 'arn:aws:iam::YOUR_ACCOUNT_ID:role/service-role/AmazonSageMaker-ExecutionRole'\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    framework_version='1.9.0',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    hyperparameters={'epochs': 3},\n",
    "    output_path=f\"s3://{bucket_name}/model-output\"\n",
    ")\n",
    "\n",
    "# Run the training job\n",
    "estimator.fit({'training': f\"s3://{bucket_name}/{file_name}\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrPPEmpFnXFo"
   },
   "outputs": [],
   "source": [
    "# Deploy SageMaker endpoint for live inference ✓ (Requirement: Live Inference)\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n",
    "\n",
    "# Batch transform for batch inference ✓ (Requirement: Batch Inference)\n",
    "transformer = estimator.transformer(instance_count=1, instance_type=\"ml.m5.large\", output_path=f\"s3://{bucket_name}/batch-output\")\n",
    "transformer.transform(data=f\"s3://{bucket_name}/{file_name}\", content_type='text/csv', split_type='Line')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mn_NoDi3nZOK"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    sagemaker = boto3.client('sagemaker')\n",
    "    sns = boto3.client('sns')\n",
    "\n",
    "    # SNS topic ARN (replace with your actual SNS topic ARN)\n",
    "    sns_topic_arn = 'arn:aws:sns:your-region:your-account-id:ModelAccuracyAlerts'\n",
    "\n",
    "    # Current accuracy passed from CloudWatch event\n",
    "    current_accuracy = float(event['current_accuracy'])\n",
    "    threshold = 0.80\n",
    "\n",
    "    if current_accuracy < threshold:\n",
    "        # Trigger model retraining\n",
    "        sagemaker.start_training_job(\n",
    "            TrainingJobName=\"retraining-job\",\n",
    "            AlgorithmSpecification={\n",
    "                'TrainingImage': estimator.training_image_uri,\n",
    "                'TrainingInputMode': 'File'\n",
    "            },\n",
    "            RoleArn=role,\n",
    "            InputDataConfig=[\n",
    "                {\n",
    "                    'ChannelName': 'training',\n",
    "                    'DataSource': {\n",
    "                        'S3DataSource': {\n",
    "                            'S3DataType': 'S3Prefix',\n",
    "                            'S3Uri': f\"s3://{bucket_name}/{file_name}\",\n",
    "                            'S3DataDistributionType': 'FullyReplicated'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            OutputDataConfig={'S3OutputPath': f\"s3://{bucket_name}/model-output\"},\n",
    "            ResourceConfig={\n",
    "                'InstanceType': 'ml.m5.large',\n",
    "                'InstanceCount': 1,\n",
    "                'VolumeSizeInGB': 50\n",
    "            },\n",
    "            StoppingCondition={'MaxRuntimeInSeconds': 3600}\n",
    "        )\n",
    "        print(\"Model retraining job triggered due to accuracy drop.\")\n",
    "\n",
    "        # Send email alert via SNS\n",
    "        sns_message = f\"Model accuracy has dropped to {current_accuracy}. Retraining has been initiated.\"\n",
    "        sns.publish(\n",
    "            TopicArn=sns_topic_arn,\n",
    "            Message=sns_message,\n",
    "            Subject=\"Model Accuracy Alert - Retraining Triggered\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Model meets accuracy threshold. No retraining required.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAyJimF-na-k"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Function to deploy the trained model on AWS SageMaker\n",
    "def deploy_model_on_aws(model_path, model_name='SentimentModel', instance_type='ml.m5.large'):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "    # Specify container image for inference (change URI if using custom images)\n",
    "    container = {\n",
    "        'Image': '763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.5.0-cpu-py36-ubuntu16.04',\n",
    "        'ModelDataUrl': model_path,\n",
    "    }\n",
    "\n",
    "    # Create the model in SageMaker\n",
    "    sagemaker_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        ExecutionRoleArn='YOUR_SAGEMAKER_EXECUTION_ROLE',\n",
    "        PrimaryContainer=container\n",
    "    )\n",
    "\n",
    "    # Deploy endpoint configuration\n",
    "    sagemaker_client.create_endpoint_config(\n",
    "        EndpointConfigName=model_name,\n",
    "        ProductionVariants=[{\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': model_name,\n",
    "            'InstanceType': instance_type,\n",
    "            'InitialInstanceCount': 1\n",
    "        }]\n",
    "    )\n",
    "\n",
    "    # Create the endpoint\n",
    "    sagemaker_client.create_endpoint(\n",
    "        EndpointName=model_name,\n",
    "        EndpointConfigName=model_name\n",
    "    )\n",
    "    print(f\"Model deployed on endpoint: {model_name}\")\n",
    "\n",
    "# Usage example:\n",
    "deploy_model_on_aws(\"s3://your-bucket/your-model-path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Enhanced monitoring function with additional metrics\n",
    "def monitor_staleness(predictions, true_labels, threshold=0.8):\n",
    "    # Calculate multiple metrics for monitoring\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n",
    "\n",
    "    # Trigger retraining if any metric falls below threshold\n",
    "    if accuracy < threshold or precision < threshold or recall < threshold:\n",
    "        print(\"Retraining triggered due to performance degradation.\")\n",
    "        # Add retraining code or call retraining function here\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=8,   # Smaller batch size for cost savings\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    save_steps=1000,                 # Save checkpoints intermittently to avoid overuse of resources\n",
    "    evaluation_strategy=\"steps\"      # Optionally, only evaluate every 'n' steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import pipeline\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load your model pipeline for text classification\n",
    "classifier = pipeline(\"text-classification\", model='distilbert-base-uncased')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    text = request.json.get('text')\n",
    "    result = classifier(text)\n",
    "    return jsonify(result)\n",
    "\n",
    "# Run the app locally for testing\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
